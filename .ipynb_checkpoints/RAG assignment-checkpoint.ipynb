{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67db59e-dc9b-4e3b-be3c-f15ae1f5c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # import hf embedding\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c9e40-d32c-4ccd-bb17-c2911ca59599",
   "metadata": {},
   "source": [
    "# Step 1: Preparing pdf metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b95856-0c9c-4ccd-8803-907aec0a65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files=[\"C:/Users/Mrinal Kalita/Python Projects/AIML Capstone Project - CV - Pneumonia Detection-1.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a41429-8752-46ba-8c88-113adae0d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_files):\n",
    "    documents = []\n",
    "    metadata = []\n",
    "    content = []\n",
    "\n",
    "    for i in pdf_files:\n",
    "\n",
    "        pdf_read = PyPDF2.PdfReader(i)\n",
    "        for ind, text in enumerate(pdf_read.pages):\n",
    "            doc_page = {'title': i + \" page \" + str(ind + 1),\n",
    "                        'content': pdf_read.pages[ind].extract_text()}\n",
    "            documents.append(doc_page)\n",
    "    for doc in documents:\n",
    "        content.append(doc[\"content\"])\n",
    "        metadata.append({\n",
    "            \"title\": doc[\"title\"]\n",
    "        })\n",
    "    print(\"Content and metadata are extracted from the documents\")\n",
    "    return content, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "957abe51-a5a9-4ae8-8f51-e190a25bc066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content and metadata are extracted from the documents\n"
     ]
    }
   ],
   "source": [
    "content, metadata = process_pdf(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c298eb-16f0-4c84-ae5f-82270e44c0a7",
   "metadata": {},
   "source": [
    "# Step 2: Split the content into smaller portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9687dc6b-377e-4370-896b-60ea98cf39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_content(content, metadata):\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=512,chunk_overlap=256)\n",
    "    smaller_docs = splitter.create_documents(content, metadatas=metadata)\n",
    "    print(f\"Docs are split into {len(smaller_docs)} passages\")\n",
    "    return smaller_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc5612ab-ccd0-4501-a9e6-b2b2bc2c5927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs are split into 7 passages\n"
     ]
    }
   ],
   "source": [
    "smaller_docs=split_content(content, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b4e6dd-465c-4572-b394-8207af633263",
   "metadata": {},
   "source": [
    "# Step 3: Ingest into Vector Database locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9a01f93-6228-4370-818f-a7bb6bf3ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_into_vectordb(smaller_docs):\n",
    "    emb = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "    db = FAISS.from_documents(smaller_docs, emb)\n",
    "\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9769f1a-e71d-4a99-9b76-1c094c03f1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x20a1cdb0ca0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_into_vectordb(smaller_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6253663-e354-40aa-bed4-86b442cd76a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
