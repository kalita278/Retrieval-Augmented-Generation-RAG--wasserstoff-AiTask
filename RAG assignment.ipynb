{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba3d831b-e2b6-496d-b792-1fef3892b98b",
   "metadata": {},
   "source": [
    "# Conversational Chatbot With Local Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee5ba4-9d54-4890-bb6e-c6b65a8616f8",
   "metadata": {},
   "source": [
    "### Content (Processess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef518acf-f444-4e07-aaa6-a2d07f03dafc",
   "metadata": {},
   "source": [
    "* PDF data processing : Extracts text ans split them into managable chunks.\n",
    "* Query Handling : Process the input questions.\n",
    "* Combining vector database and LLMs: We leverage langchain's capabilities to link vector database indexing with llama-2 LLMs, enabling a seamless conversational experience with memory and retrieval functionalities.\n",
    "* Hallucination Check: Also detects any inaccuracy or hallucination occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6098d-6011-4f3c-b1a8-d2e360365ae0",
   "metadata": {},
   "source": [
    "# Pre-requisite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7760c-bf03-426f-899a-e6bdb2a2c604",
   "metadata": {},
   "source": [
    "* Install all the libraries from the requirements.txt file using \"pip install -r requirements.txt\" cammand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67db59e-dc9b-4e3b-be3c-f15ae1f5c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import LlamaCpp\n",
    "#from langchain.llms import LlamaCpp\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, GPT4AllEmbeddings # import hf embedding\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c9e40-d32c-4ccd-bb17-c2911ca59599",
   "metadata": {},
   "source": [
    "# Step 1: Preparing pdf metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b95856-0c9c-4ccd-8803-907aec0a65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files=[\"C:/Users/Mrinal Kalita/Python Projects/AIML Capstone Project - CV - Pneumonia Detection-1.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a41429-8752-46ba-8c88-113adae0d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_files):\n",
    "    documents = []\n",
    "    metadata = []\n",
    "    content = []\n",
    "\n",
    "    for i in pdf_files:\n",
    "\n",
    "        pdf_read = PyPDF2.PdfReader(i)\n",
    "        for ind, text in enumerate(pdf_read.pages):\n",
    "            doc_page = {'title': i + \" page \" + str(ind + 1),\n",
    "                        'content': pdf_read.pages[ind].extract_text()}\n",
    "            documents.append(doc_page)\n",
    "    for doc in documents:\n",
    "        content.append(doc[\"content\"])\n",
    "        metadata.append({\n",
    "            \"title\": doc[\"title\"]\n",
    "        })\n",
    "    print(\"Content and metadata are extracted from the documents\")\n",
    "    return content, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957abe51-a5a9-4ae8-8f51-e190a25bc066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content and metadata are extracted from the documents\n"
     ]
    }
   ],
   "source": [
    "content, metadata = process_pdf(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c298eb-16f0-4c84-ae5f-82270e44c0a7",
   "metadata": {},
   "source": [
    "# Step 2: Split the content into smaller portion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46776f66-17ef-4ddb-b569-0918fe9eb9e5",
   "metadata": {},
   "source": [
    "The split_content function takes text content and metadata as inputs and splits the content into smaller portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9687dc6b-377e-4370-896b-60ea98cf39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_content(content, metadata):\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=512,chunk_overlap=256)\n",
    "    smaller_docs = splitter.create_documents(content, metadatas=metadata)\n",
    "    print(f\"Docs are split into {len(smaller_docs)} passages\")\n",
    "    return smaller_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5612ab-ccd0-4501-a9e6-b2b2bc2c5927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs are split into 7 passages\n"
     ]
    }
   ],
   "source": [
    "smaller_docs=split_content(content, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b4e6dd-465c-4572-b394-8207af633263",
   "metadata": {},
   "source": [
    "# Step 3: Ingest into Vector Database locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1725d8-1eb5-465d-86d1-06318c7c1bba",
   "metadata": {},
   "source": [
    "The ingest_into_vectordb function is designed for processing and indexing a collection of documents into a vector database using FAISS (Facebook AI Similarity Search) for efficient similarity searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a01f93-6228-4370-818f-a7bb6bf3ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_into_vectordb(smaller_docs):\n",
    "    emb = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "    #emb = GPT4AllEmbeddings(model_name='all-MiniLM-L6-v2.gguf2.f16.gguf', gpt4all_kwargs={'allow_download': 'True'})\n",
    "    db = FAISS.from_documents(smaller_docs, emb)\n",
    "\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9769f1a-e71d-4a99-9b76-1c094c03f1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca1fec985314ad4b4bdb75c65918f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\Transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Mrinal Kalita\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a5e3fa8342448fa792e7d5d9ad342a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c11c9ea2a2b426a9ae8d716cf5e3e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6925ac58930e45f6b512b597cb52b8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d18f7beaa841c9ad6fc1b5de4ca8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66ed0a3d19d454aacc1d9948395b002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ccd2d212af48c2acf9e29c50b1d21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37aac619fcee4068928caa5aa049d5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20c91fe163d49659af4d9562c614960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dffa5f9a1704cb6a3569dcadb99aa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8246fa97ddc745df88063a8d6e7844d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector =ingest_into_vectordb(smaller_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b809e99-b41e-473b-9a21-e13c77cca4be",
   "metadata": {},
   "source": [
    "# Step4 : LLM Prompt conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e758c83-e369-4aa5-8c7f-298636fd7aa2",
   "metadata": {},
   "source": [
    "The conversation_func function is designed to create and configure a conversational chain for a language model, specifically using the LLaMA model and a vector database for retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d284b1-46ff-46d4-b397-b4caa8b60747",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"[INST]\n",
    "As an AI expert, based on the provided document,please provide accurate, important and relevant information. Your responses should follow the following guidelines:\n",
    "- Answer the question based on the provided documents.\n",
    "- Be direct, factual and precise while answering, limited to 50 words and 2-3 sentences. Begin your response without using introductory phrases like yes, no etc.\n",
    "- Maintain an ethical, unbiased and neutral tone, avoiding harmful or offensive content.\n",
    "- If the document does not contain relevant information, state \"The document doesn't have any relevent information avilable.\"\n",
    "- Do not include questions in your responses.\n",
    "- Answer the questions directly. do not ask me questions\n",
    "{question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "#template = \"\"\"Given the document and the current conversation between a user and an agent, your task is as follows: Answer any user query by using information from the document. The response should be detailed.\"\"\"\n",
    "callback = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "def conversation_func(vector):\n",
    "    llama_llm = LlamaCpp(\n",
    "    model_path=\"C:/Users/Mrinal Kalita/langchain-notes/mistral-7b-openorca.gguf2.Q4_0.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=200,\n",
    "    top_p=1,\n",
    "    callback_manager=callback,\n",
    "    n_ctx=3000)\n",
    "\n",
    "    retriever = vector.as_retriever()\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "\n",
    "    conversation_chat = (ConversationalRetrievalChain.from_llm\n",
    "                          (llm=llama_llm,\n",
    "                           retriever=retriever,\n",
    "                           #condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                           memory=memory,\n",
    "                           return_source_documents=True))\n",
    "    print(\"Conversation function created for the LLM using the vector store\")\n",
    "    return conversation_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f32d4f9d-4e2d-432e-9ac0-d38c27e95833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:/Users/Mrinal Kalita/langchain-notes/mistral-7b-openorca.gguf2.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Open-Orca_Mistral-7B-OpenOrca\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = Open-Orca_Mistral-7B-OpenOrca\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3917.88 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 3008\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   376.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  376.00 MiB, K (f16):  188.00 MiB, V (f16):  188.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    14.12 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation function created for the LLM using the vector store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'Open-Orca_Mistral-7B-OpenOrca', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '2', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "con = conversation_func(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4519658-acaa-4a34-a21d-3a80b6725d86",
   "metadata": {},
   "source": [
    "# Step 5: etect Hallucination in the LLMs Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c64cc-dc89-4d81-a59c-37f4fc2af9ef",
   "metadata": {},
   "source": [
    "The validate_answer_against_sources function evaluates the reliability of a response by comparing it with source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34c172ba-3c21-4e49-836c-4b69280c0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_answer_against_sources(response_answer, source_documents):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    similarity_threshold = 0.5  \n",
    "    source_texts = [doc.page_content for doc in source_documents]\n",
    "\n",
    "    answer_embedding = model.encode(response_answer, convert_to_tensor=True)\n",
    "    source_embeddings = model.encode(source_texts, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.pytorch_cos_sim(answer_embedding, source_embeddings)\n",
    "\n",
    "\n",
    "    if any(score.item() > similarity_threshold for score in cosine_scores[0]):\n",
    "        return True  \n",
    "\n",
    "    return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb54592-547b-4ad0-9513-90b42fc3a083",
   "metadata": {},
   "source": [
    "# Asking Quetions to chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f62dda0-123e-4562-a1fa-b8eb851817ef",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "822af0d7-cbbc-4a39-bed3-bbd79cc2bd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\Transformers\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The objective of this project is to design a DL based algorithm for detecting pneumonia."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2894.31 ms\n",
      "llama_print_timings:      sample time =       5.94 ms /    20 runs   (    0.30 ms per token,  3369.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =  369859.38 ms /   992 tokens (  372.84 ms per token,     2.68 tokens per second)\n",
      "llama_print_timings:        eval time =    7729.95 ms /    20 runs   (  386.50 ms per token,     2.59 tokens per second)\n",
      "llama_print_timings:       total time =  377675.37 ms /  1012 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  what is the objective of this project?\n",
      "A:   The objective of this project is to design a DL based algorithm for detecting pneumonia.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"what is the objective of this project?\"\n",
    "response=con({\"question\": user_question})\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79ad7c02-9b27-4715-8fff-38412afeaa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  what is the objective of this project?\n",
      "A:   The objective of this project is to design a DL based algorithm for detecting pneumonia.\n"
     ]
    }
   ],
   "source": [
    "if response['source_documents']:\n",
    "    response_answer = response['answer']\n",
    "    source_docs = response['source_documents']\n",
    "\n",
    "    # Post-processing step to validate the answer against the source documents\n",
    "    is_valid_answer = validate_answer_against_sources(response_answer, source_docs)\n",
    "    if not is_valid_answer:\n",
    "        response['answer'] = \"Sorry I can not answer the question based on the given documents\"\n",
    "else:\n",
    "    response['answer'] =\"Sorry, I cannot answer the question based on the given documents\"\n",
    "\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61deffa-1fb5-4b0b-805b-50e6d938b886",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f847874-3d81-46b5-8386-4f37324ca888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Can you please provide an overview of the steps involved in the project?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2894.31 ms\n",
      "llama_print_timings:      sample time =       4.16 ms /    15 runs   (    0.28 ms per token,  3607.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27064.44 ms /    88 tokens (  307.55 ms per token,     3.25 tokens per second)\n",
      "llama_print_timings:        eval time =    6551.60 ms /    14 runs   (  467.97 ms per token,     2.14 tokens per second)\n",
      "llama_print_timings:       total time =   33661.22 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The project involves three milestones. Milestone 1 is worth 40 points and focuses on importing data, mapping training and testing images to their classes and annotations, preprocessing and visualization, displaying images with bounding boxes, and designing, training, and testing basic CNN models for classification. Milestone 2, worth 60 points, involves fine-tuning trained CNN models, applying transfer learning models, designing and testing RCNN and its hybrids based object detection models to impose bounding boxes or masks, pickling the model for future prediction, and final report submission. Milestone 3 is optional and involves creating a clickable UI-based interface that allows users to input images, output classes, and bounding boxes or masks."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2894.31 ms\n",
      "llama_print_timings:      sample time =      46.83 ms /   159 runs   (    0.29 ms per token,  3395.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  422851.76 ms /  1347 tokens (  313.92 ms per token,     3.19 tokens per second)\n",
      "llama_print_timings:        eval time =   70556.20 ms /   158 runs   (  446.56 ms per token,     2.24 tokens per second)\n",
      "llama_print_timings:       total time =  493956.45 ms /  1505 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  How many steps are there in the project\n",
      "A:   The project involves three milestones. Milestone 1 is worth 40 points and focuses on importing data, mapping training and testing images to their classes and annotations, preprocessing and visualization, displaying images with bounding boxes, and designing, training, and testing basic CNN models for classification. Milestone 2, worth 60 points, involves fine-tuning trained CNN models, applying transfer learning models, designing and testing RCNN and its hybrids based object detection models to impose bounding boxes or masks, pickling the model for future prediction, and final report submission. Milestone 3 is optional and involves creating a clickable UI-based interface that allows users to input images, output classes, and bounding boxes or masks.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How many steps are there in the project\"\n",
    "response=con({\"question\": user_question})\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b26d29c-b7f0-495e-b9ac-9aa9caf148b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  How many steps are there in the project\n",
      "A:   The project involves three milestones. Milestone 1 is worth 40 points and focuses on importing data, mapping training and testing images to their classes and annotations, preprocessing and visualization, displaying images with bounding boxes, and designing, training, and testing basic CNN models for classification. Milestone 2, worth 60 points, involves fine-tuning trained CNN models, applying transfer learning models, designing and testing RCNN and its hybrids based object detection models to impose bounding boxes or masks, pickling the model for future prediction, and final report submission. Milestone 3 is optional and involves creating a clickable UI-based interface that allows users to input images, output classes, and bounding boxes or masks.\n"
     ]
    }
   ],
   "source": [
    "if response['source_documents']:\n",
    "    response_answer = response['answer']\n",
    "    source_docs = response['source_documents']\n",
    "\n",
    "    # Post-processing step to validate the answer against the source documents\n",
    "    is_valid_answer = validate_answer_against_sources(response_answer, source_docs)\n",
    "    if not is_valid_answer:\n",
    "        response['answer'] = \"Sorry I can not answer the question based on the given documents\"\n",
    "else:\n",
    "    response['answer'] =\"Sorry, I cannot answer the question based on the given documents\"\n",
    "\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f02602-13cf-4601-8922-91d267609dc6",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60cb7000-3dac-4404-94fb-e749d65e8e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Who was Albert Einstein?\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2894.31 ms\n",
      "llama_print_timings:      sample time =       1.95 ms /     7 runs   (    0.28 ms per token,  3587.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   67622.65 ms /   260 tokens (  260.09 ms per token,     3.84 tokens per second)\n",
      "llama_print_timings:        eval time =    2575.24 ms /     6 runs   (  429.21 ms per token,     2.33 tokens per second)\n",
      "llama_print_timings:       total time =   70221.83 ms /   266 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Albert Einstein was a German-born physicist who is best known for his theory of relativity and his famous equation E=mc². He made significant contributions to the field of physics and is considered one of the most influential scientists in history."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2894.31 ms\n",
      "llama_print_timings:      sample time =      14.18 ms /    50 runs   (    0.28 ms per token,  3526.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  450300.61 ms /  1255 tokens (  358.81 ms per token,     2.79 tokens per second)\n",
      "llama_print_timings:        eval time =   23405.70 ms /    49 runs   (  477.67 ms per token,     2.09 tokens per second)\n",
      "llama_print_timings:       total time =  473882.47 ms /  1304 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  Who is Elber Einstein\n",
      "A:   Albert Einstein was a German-born physicist who is best known for his theory of relativity and his famous equation E=mc². He made significant contributions to the field of physics and is considered one of the most influential scientists in history.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Who is Elber Einstein\"\n",
    "response=con({\"question\": user_question})\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb81ad52-eef6-4ee2-a7ee-80cb15ee6f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  Who is Elber Einstein\n",
      "A:  Sorry I can not answer the question based on the given documents\n"
     ]
    }
   ],
   "source": [
    "if response['source_documents']:\n",
    "    response_answer = response['answer']\n",
    "    source_docs = response['source_documents']\n",
    "\n",
    "    # Post-processing step to validate the answer against the source documents\n",
    "    is_valid_answer = validate_answer_against_sources(response_answer, source_docs)\n",
    "    if not is_valid_answer:\n",
    "        response['answer'] = \"Sorry I can not answer the question based on the given documents\"\n",
    "else:\n",
    "    response['answer'] =\"Sorry, I cannot answer the question based on the given documents\"\n",
    "\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb07a3a7-1cd3-435f-8b88-4f1c5af2e3a7",
   "metadata": {},
   "source": [
    "We can see that, the chatbot is working well. Since the question no 3 is out of context for the document. So it responded with the out of context guideline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de9155-f9ce-4378-8ee8-62938ab49b1f",
   "metadata": {},
   "source": [
    "# Running The Streamlit App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e0047-acbc-4d15-9701-f11ee277f4b0",
   "metadata": {},
   "source": [
    "To run the streamlit app in your local system - Go to command propt and run \"streamlit run app.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208cc8a-858d-43d9-80ac-7c4c8a24d63c",
   "metadata": {},
   "source": [
    "# Challenges Encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772255a9-51c9-4f4b-91e2-dccdf800542b",
   "metadata": {},
   "source": [
    "* Less Computer resource availability.\n",
    "* Some LLMs are paid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6f46c-8d5f-4dea-9632-f3f8895d69e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
