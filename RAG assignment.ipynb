{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67db59e-dc9b-4e3b-be3c-f15ae1f5c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # import hf embedding\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c9e40-d32c-4ccd-bb17-c2911ca59599",
   "metadata": {},
   "source": [
    "# Step 1: Preparing pdf metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b95856-0c9c-4ccd-8803-907aec0a65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files=[\"C:/Users/Mrinal Kalita/Python Projects/AIML Capstone Project - CV - Pneumonia Detection-1.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a41429-8752-46ba-8c88-113adae0d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_files):\n",
    "    documents = []\n",
    "    metadata = []\n",
    "    content = []\n",
    "\n",
    "    for i in pdf_files:\n",
    "\n",
    "        pdf_read = PyPDF2.PdfReader(i)\n",
    "        for ind, text in enumerate(pdf_read.pages):\n",
    "            doc_page = {'title': i + \" page \" + str(ind + 1),\n",
    "                        'content': pdf_read.pages[ind].extract_text()}\n",
    "            documents.append(doc_page)\n",
    "    for doc in documents:\n",
    "        content.append(doc[\"content\"])\n",
    "        metadata.append({\n",
    "            \"title\": doc[\"title\"]\n",
    "        })\n",
    "    print(\"Content and metadata are extracted from the documents\")\n",
    "    return content, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957abe51-a5a9-4ae8-8f51-e190a25bc066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content and metadata are extracted from the documents\n"
     ]
    }
   ],
   "source": [
    "content, metadata = process_pdf(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c298eb-16f0-4c84-ae5f-82270e44c0a7",
   "metadata": {},
   "source": [
    "# Step 2: Split the content into smaller portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9687dc6b-377e-4370-896b-60ea98cf39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_content(content, metadata):\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=512,chunk_overlap=256)\n",
    "    smaller_docs = splitter.create_documents(content, metadatas=metadata)\n",
    "    print(f\"Docs are split into {len(smaller_docs)} passages\")\n",
    "    return smaller_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5612ab-ccd0-4501-a9e6-b2b2bc2c5927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs are split into 7 passages\n"
     ]
    }
   ],
   "source": [
    "smaller_docs=split_content(content, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b4e6dd-465c-4572-b394-8207af633263",
   "metadata": {},
   "source": [
    "# Step 3: Ingest into Vector Database locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a01f93-6228-4370-818f-a7bb6bf3ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_into_vectordb(smaller_docs):\n",
    "    emb = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "    db = FAISS.from_documents(smaller_docs, emb)\n",
    "\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9769f1a-e71d-4a99-9b76-1c094c03f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector =ingest_into_vectordb(smaller_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b809e99-b41e-473b-9a21-e13c77cca4be",
   "metadata": {},
   "source": [
    "# Step4 : LLM Prompt conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d284b1-46ff-46d4-b397-b4caa8b60747",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"[INST]\n",
    "As an AI expert, based on the provided document,please provide accurate, important and relevant information. Your responses should follow the following guidelines:\n",
    "- Answer the question based on the provided documents.\n",
    "- Be direct, factual and precise while answering, limited to 50 words and 2-3 sentences. Begin your response without using introductory phrases like yes, no etc.\n",
    "- Maintain an ethical, unbiased and neutral tone, avoiding harmful or offensive content.\n",
    "- If the document does not contain relevant information, state \"The document doesn't have any relevent information avilable.\"\n",
    "- Do not include questions in your responses.\n",
    "- Answer the questions directly. do not ask me questions\n",
    "{question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "#template = \"\"\"Given the document and the current conversation between a user and an agent, your task is as follows: Answer any user query by using information from the document. The response should be detailed.\"\"\"\n",
    "callback = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "def conversation_func(vector):\n",
    "    llama_llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=200,\n",
    "    top_p=1,\n",
    "    callback_manager=callback,\n",
    "    n_ctx=3000)\n",
    "\n",
    "    retriever = vector.as_retriever()\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='history', return_messages=True, output_key='answer')\n",
    "\n",
    "    conversation_chat = (ConversationalRetrievalChain.from_llm\n",
    "                          (llm=llama_llm,\n",
    "                           retriever=retriever,\n",
    "                           #condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                           memory=memory,\n",
    "                           return_source_documents=True))\n",
    "    print(\"Conversation function created for the LLM using the vector store\")\n",
    "    return conversation_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f32d4f9d-4e2d-432e-9ac0-d38c27e95833",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LlamaCpp\n__root__\n  Could not load Llama model from path: llama-2-7b-chat.Q4_K_M.gguf. Received error Model path does not exist: llama-2-7b-chat.Q4_K_M.gguf (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mconversation_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m, in \u001b[0;36mconversation_func\u001b[1;34m(vector)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconversation_func\u001b[39m(vector):\n\u001b[1;32m---> 16\u001b[0m     llama_llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-2-7b-chat.Q4_K_M.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     retriever \u001b[38;5;241m=\u001b[39m vector\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[0;32m     25\u001b[0m     CONDENSE_QUESTION_PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Transformers\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LlamaCpp\n__root__\n  Could not load Llama model from path: llama-2-7b-chat.Q4_K_M.gguf. Received error Model path does not exist: llama-2-7b-chat.Q4_K_M.gguf (type=value_error)"
     ]
    }
   ],
   "source": [
    "conversation_func(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33beefb3-d065-4238-a6e6-46f569138710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
